{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [WIP] My approach at solving the research problem of AI`nspired project with DINOV2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you want to run this on your machine, be sure to download a pretrained backbone from official DINOV2 repo: https://github.com/facebookresearch/dinov2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.ndimage import binary_closing, binary_opening\n",
    "from typing import Tuple\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "from utilities import *\n",
    "from dinov2.models.vision_transformer import vit_small, vit_base, vit_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick overview**:\n",
    " - MISSING 6A in WEB and AI\n",
    " - MISSING 12B in AI\n",
    " - MISSING 22B in WEB and AI\n",
    " - MISSING 23B in AI\n",
    " - MISSING 26 IN WEB and AI (!) - because of this we need to skip group 26 for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = get_groups()\n",
    "final_data = pd.concat([groups.iloc[:25], groups[26:]], axis=0)\n",
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if everything loaded properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_data)):\n",
    "    print(f'Sample photos from group {final_data.iloc[i].group_code}')\n",
    "    ai = cv2.imread(final_data.iloc[i].ai_images[1])\n",
    "    web = cv2.imread(final_data.iloc[i].web_images[1])\n",
    "    final = cv2.imread(final_data.iloc[i].final_submissions[1])\n",
    "    ai_aggregated_similarity, web_aggregated_similarity = 0, 0\n",
    "    max_similarity, picture1, picture2 = 0, \"\", \"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(ai)\n",
    "    plt.title(\"AI\")\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(web)\n",
    "    plt.title(\"WEB\")\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(final)\n",
    "    plt.title(\"Submission\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DINOV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISCLAIMER: This notebook will not run on your computer. I had to modify DinoV2 manually on my computer to make it work. I created my own fork of the original repository and I'll adapt the code to load the model from my repository later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEFAULT_BACKGROUND_THRESHOLD = 0.05\n",
    "DEFAULT_APPLY_OPENING = False\n",
    "DEFAULT_APPLY_CLOSING = False\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoV2():\n",
    "    \n",
    "    def __init__(self, \n",
    "                checkpoint: str ='dinov2_vitb14_reg4_pretrain.pth', \n",
    "                patch_size: int = 14, \n",
    "                img_size: int = 526, \n",
    "                n_register_tokens: int = 4, \n",
    "                smaller_edge_size: int = 224, \n",
    "                device=DEVICE\n",
    "                ):\n",
    "        self.model = vit_base(\n",
    "            patch_size=patch_size,\n",
    "            img_size=img_size,\n",
    "            init_values=1.0,\n",
    "            num_register_tokens=n_register_tokens,\n",
    "            block_chunks=0\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "        self.smaller_edge = smaller_edge_size\n",
    "        self.n_register_tokens = n_register_tokens\n",
    "        self.device = device\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(size=self.smaller_edge, interpolation=transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), # imagenet defaults\n",
    "        ])\n",
    "        self.model.load_state_dict(torch.load(checkpoint, map_location=device, weights_only=True))\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def prepare_image(self, rgb_image_numpy):\n",
    "        with torch.inference_mode():\n",
    "            image = Image.fromarray(rgb_image_numpy)\n",
    "            image_tensor = self.transform(Image.fromarray(rgb_image_numpy))\n",
    "            resize_scale = image.width / image_tensor.shape[2]\n",
    "            del rgb_image_numpy\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Crop image to dimensions that are a multiple of the patch size\n",
    "        height, width = image_tensor.shape[1:] # C x H x W\n",
    "        cropped_width, cropped_height = width - width % self.patch_size, height - height % self.patch_size # crop a bit from right and bottom parts\n",
    "        image_tensor = image_tensor[:, :cropped_height, :cropped_width]\n",
    "        grid_size = (cropped_height // self.patch_size, cropped_width // self.patch_size)\n",
    "            \n",
    "        return image_tensor, grid_size, resize_scale\n",
    "    \n",
    "    def prepare_mask(self, mask_image_numpy, grid_size, resize_scale):\n",
    "        cropped_mask_image_numpy = mask_image_numpy[:int(grid_size[0]*self.model.patch_size*resize_scale), :int(grid_size[1]*self.model.patch_size*resize_scale)]\n",
    "        image = Image.fromarray(cropped_mask_image_numpy)\n",
    "        resized_mask = image.resize((grid_size[1], grid_size[0]), resample=Image.Resampling.NEAREST)\n",
    "        resized_mask = np.asarray(resized_mask).flatten()\n",
    "        return resized_mask\n",
    "\n",
    "    def idx_to_source_position(self, idx, grid_size, resize_scale):\n",
    "        row = (idx // grid_size[1])*self.model.patch_size*resize_scale + self.model.patch_size / 2\n",
    "        col = (idx % grid_size[1])*self.model.patch_size*resize_scale + self.model.patch_size / 2\n",
    "        return row, col\n",
    "  \n",
    "    def get_embedding_visualization(self, tokens, grid_size, resized_mask=None):\n",
    "        pca = PCA(n_components=3)\n",
    "        if resized_mask is not None:\n",
    "            tokens = tokens[resized_mask]\n",
    "        reduced_tokens = pca.fit_transform(tokens.astype(np.float32))\n",
    "        if resized_mask is not None:\n",
    "            tmp_tokens = np.zeros((*resized_mask.shape, 3), dtype=reduced_tokens.dtype)\n",
    "            tmp_tokens[resized_mask] = reduced_tokens\n",
    "            reduced_tokens = tmp_tokens\n",
    "        reduced_tokens = reduced_tokens.reshape((*grid_size, -1))\n",
    "        normalized_tokens = (reduced_tokens-np.min(reduced_tokens))/(np.max(reduced_tokens)-np.min(reduced_tokens))\n",
    "        return normalized_tokens\n",
    "\n",
    "    def extract_features(self, image_numpy, pooling: bool = True):\n",
    "        with torch.inference_mode():\n",
    "            image_tensor = self.prepare_image(image_numpy)[0]\n",
    "            image_tensor = image_tensor.unsqueeze(0).to(self.device)\n",
    "\n",
    "            tokens = self.model.get_intermediate_layers(image_tensor)[0].squeeze()\n",
    "            del image_tensor, image_numpy\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if pooling == False:\n",
    "                return tokens.cpu().numpy()\n",
    "\n",
    "            pooled_features = tokens.mean(dim=0)\n",
    "            del tokens\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            return pooled_features\n",
    "\n",
    "    def calculate_similarity(self, image1: str, image2: str):\n",
    "        with torch.inference_mode():\n",
    "            features1 = self.extract_features(cv2.cvtColor(cv2.imread(image1, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB))\n",
    "            features2 = self.extract_features(cv2.cvtColor(cv2.imread(image2, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            similarity = F.cosine_similarity(features1, features2, dim=0)\n",
    "            del features1, features2\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            return (similarity.item() + 1) / 2\n",
    "\n",
    "    def create_attention_mask(self, image_metric, save: bool = False, show: bool = False):\n",
    "        with torch.inference_mode():\n",
    "            normalized_metric = Normalize(vmin=image_metric.min(), vmax=image_metric.max())(image_metric)\n",
    "            del image_metric\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Apply the Reds colormap\n",
    "            reds = plt.cm.Reds(normalized_metric)\n",
    "\n",
    "            # Create the alpha channel\n",
    "            alpha_max_value = 1.00  # Set your max alpha value\n",
    "\n",
    "            # Adjust this value as needed to enhance lower values visibility\n",
    "            gamma = 0.5  \n",
    "\n",
    "            # Apply gamma transformation to enhance lower values\n",
    "            enhanced_metric = np.power(normalized_metric, gamma)\n",
    "            del normalized_metric, gamma\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Create the alpha channel with enhanced visibility for lower values\n",
    "            alpha_channel = enhanced_metric * alpha_max_value\n",
    "\n",
    "            # Add the alpha channel to the RGB data\n",
    "            rgba_mask = np.zeros((enhanced_metric.shape[0], enhanced_metric.shape[1], 4))\n",
    "            rgba_mask[..., :3] = reds[..., :3]  # RGB\n",
    "            rgba_mask[..., 3] = alpha_channel  # Alpha\n",
    "            del reds, alpha_max_value, enhanced_metric, alpha_channel\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Convert the numpy array to PIL Image\n",
    "            rgba_image = Image.fromarray((rgba_mask * 255).astype(np.uint8))\n",
    "            del rgba_mask\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if save:\n",
    "                rgba_image.save('attention_mask.png')\n",
    "            if show:\n",
    "                display(rgba_image)\n",
    "\n",
    "            return rgba_image\n",
    "\n",
    "    def create_attention_photo(self, og_image: Image, attention_mask_image, save: bool = False, show: bool = False):\n",
    "        # Ensure both images are in the same mode\n",
    "        if og_image.mode != 'RGBA':\n",
    "            og_image = og_image.convert('RGBA')\n",
    "\n",
    "        # Overlay the second image onto the first image\n",
    "        # The second image must be the same size as the first image\n",
    "        og_image.paste(attention_mask_image, (0, 0), attention_mask_image)\n",
    "\n",
    "        if save:\n",
    "            og_image.save('image_with_attention.png')\n",
    "        if show:\n",
    "            display(og_image)\n",
    "\n",
    "        return og_image\n",
    "\n",
    "    def return_attention_map(self, filepath: str, show: bool = False, mask_only: bool = False):\n",
    "        with torch.inference_mode():\n",
    "            # I know this is a weird way to do this but it works for now\n",
    "            og_image = Image.open(filepath)\n",
    "            (original_w, original_h) = og_image.size\n",
    "\n",
    "            if show:\n",
    "                display(og_image)\n",
    "\n",
    "            img = self.prepare_image(cv2.cvtColor(cv2.imread(filepath, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB))[0]\n",
    "            w, h = img.shape[1] - img.shape[1] % self.patch_size, img.shape[2] - img.shape[2] % self.patch_size\n",
    "            img = img[:, :w, :h]\n",
    "\n",
    "            w_featmap = img.shape[-2] // self.patch_size\n",
    "            h_featmap = img.shape[-1] // self.patch_size\n",
    "\n",
    "            img = img.unsqueeze(0)\n",
    "            img = img.to(self.device)\n",
    "            attention = self.model.get_last_self_attention(img.to(self.device))\n",
    "            del img, w, h\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            number_of_heads = attention.shape[1]\n",
    "\n",
    "            # attention tokens are packed in after the first token; the spatial tokens follow\n",
    "            attention = attention[0, :, 0, 1 + self.n_register_tokens:].reshape(number_of_heads, -1)\n",
    "\n",
    "            # resolution of attention from transformer tokens\n",
    "            attention = attention.reshape(number_of_heads, w_featmap, h_featmap)\n",
    "            \n",
    "            # upscale to higher resolution closer to original image\n",
    "            attention = nn.functional.interpolate(attention.unsqueeze(0), scale_factor=self.patch_size, mode = \"nearest\")[0].cpu()\n",
    "\n",
    "            # sum all attention across the 12 different heads, to get one map of attention across entire image\n",
    "            attention = torch.sum(attention, dim=0)\n",
    "\n",
    "            # interpolate attention map back into original image dimensions\n",
    "            attention = nn.functional.interpolate(attention.unsqueeze(0).unsqueeze(0), size=(original_h, original_w), mode='bilinear', align_corners=False)\n",
    "            del original_h, original_w, w_featmap, h_featmap, number_of_heads\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            attention = attention.squeeze()\n",
    "            image_metric = attention.numpy()\n",
    "            del attention\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            attention_mask = self.create_attention_mask(image_metric, show=show)\n",
    "            del image_metric\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if mask_only:\n",
    "                return attention_mask\n",
    "            \n",
    "            photo_with_attention = self.create_attention_photo(og_image, attention_mask, show=show)\n",
    "            del og_image\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            return attention_mask, photo_with_attention\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small demo of the attention maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/ai/1B_15_4.png'\n",
    "with torch.no_grad():\n",
    "    dino = DinoV2()\n",
    "    attention_mask, attention_map = dino.return_attention_map(file)\n",
    "    display(attention_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lines(model, image1, image2, origin, similarity_threshold=0.1, max_lines=20, n_neighbors=1):\n",
    "    img1 = cv2.cvtColor(cv2.imread(image1, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "    img2 = cv2.cvtColor(cv2.imread(image2, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    _, grid_size1, resize_scale1 = model.prepare_image(img1)\n",
    "    _, grid_size2, resize_scale2 = model.prepare_image(img2)\n",
    "\n",
    "    features1 = model.extract_features(img1, pooling=False)\n",
    "    features2 = model.extract_features(img2, pooling=False)\n",
    "    \n",
    "    # Use multiple neighbors\n",
    "    knn = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    knn.fit(features1)\n",
    "    distances, matches = knn.kneighbors(features2)\n",
    "\n",
    "    # Normalize distances to [0, 1]\n",
    "    distances = (distances - distances.min()) / (distances.max() - distances.min())\n",
    "\n",
    "    # Filter and select matches based on similarity threshold\n",
    "    selected_matches = []\n",
    "    for idx2, (dist_row, match_row) in enumerate(zip(distances, matches)):\n",
    "        for dist, idx1 in zip(dist_row, match_row):\n",
    "            if dist < similarity_threshold:\n",
    "                selected_matches.append((dist, idx1, idx2))\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))  \n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax1.imshow(img1)\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.set_title(\"Final submission\")\n",
    "    ax2.imshow(img2)\n",
    "    ax2.axis(\"off\")\n",
    "    ax2.set_title(f\"Closest inspiration - {origin}\")\n",
    "\n",
    "    if len(selected_matches) != 0:\n",
    "        # Normalize selected matches distances\n",
    "        match_distances = [match[0] for match in selected_matches]\n",
    "        normalized_distances = (match_distances - np.min(match_distances)) / (np.max(match_distances) - np.min(match_distances) + 1e-8)\n",
    "\n",
    "        # Update selected_matches with normalized distances\n",
    "        for i, match in enumerate(selected_matches):\n",
    "            selected_matches[i] = (normalized_distances[i], match[1], match[2])\n",
    "\n",
    "        # Sort and limit number of lines\n",
    "        selected_matches = sorted(selected_matches, key=lambda x: x[0])[:max_lines]\n",
    "\n",
    "        for dist, idx1, idx2 in selected_matches:\n",
    "            enhanced_dist = np.sqrt(dist)\n",
    "            row, col = model.idx_to_source_position(idx1, grid_size1, resize_scale1)\n",
    "            xyA = (col, row)\n",
    "\n",
    "            row, col = model.idx_to_source_position(idx2, grid_size2, resize_scale2)\n",
    "            xyB = (col, row)\n",
    "\n",
    "            # Map similarity to color and thickness\n",
    "            color = cm.plasma(enhanced_dist)  # Colormap based on similarity\n",
    "            linewidth = (1 + enhanced_dist)  # Thicker lines for higher similarity\n",
    "            \n",
    "            con = ConnectionPatch(xyA=xyB, xyB=xyA, coordsA=\"data\", coordsB=\"data\",\n",
    "                                axesA=ax2, axesB=ax1, color=color, linewidth=linewidth)\n",
    "            ax2.add_artist(con)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def draw_attention(model, pic1, pic2):\n",
    "    for picture in [pic1, pic2]:\n",
    "        attn_mask, attn_photo = model.return_attention_map(picture, show=False)\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "                \n",
    "        ax1 = fig.add_subplot(131)\n",
    "        ax2 = fig.add_subplot(132)\n",
    "        ax3 = fig.add_subplot(133)\n",
    "        \n",
    "        ax1.imshow(cv2.cvtColor(cv2.imread(picture, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB))\n",
    "        ax1.set_title(\"Photo\")\n",
    "        ax1.axis(\"off\")\n",
    "                \n",
    "        ax2.imshow(attn_mask)\n",
    "        ax2.set_title(\"Attention Mask\")\n",
    "        ax2.axis(\"off\")\n",
    "                \n",
    "        ax3.imshow(attn_photo)\n",
    "        ax3.set_title(\"Masked Photo\")\n",
    "        ax3.axis(\"off\")\n",
    "                \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see if the similarity scores make any sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 12\n",
    "\n",
    "with torch.inference_mode():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ai = final_data.iloc[group].ai_images\n",
    "    web = final_data.iloc[group].web_images\n",
    "    final = final_data.iloc[group].final_submissions\n",
    "    max_ai, min_ai, max_web, min_web = -1, float('inf'), -1, float('inf')\n",
    "    ai_total, web_total = 0, 0\n",
    "    max_similarity, pic1, pic2 = -1, \"\", \"\"\n",
    "    inspiration = \"INCONCLUSIVE\"\n",
    "\n",
    "    print(f'Calculating similarity for group {final_data.iloc[group].group_code}')\n",
    "\n",
    "    for final_photo in final:\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"AI PHOTOS\")\n",
    "        for ai_photo in ai:\n",
    "            similarity = dino.calculate_similarity(final_photo, ai_photo)\n",
    "            print(f\"Similarity: {similarity}\")\n",
    "\n",
    "            min_ai = min(min_ai, similarity)\n",
    "            max_ai = max(max_ai, similarity)\n",
    "\n",
    "            draw_attention(dino, final_photo, ai_photo)\n",
    "            draw_lines(dino, final_photo, ai_photo, \"AI\")\n",
    "\n",
    "            ai_total += similarity\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                pic1, pic2 = final_photo, ai_photo\n",
    "                if similarity > 0.5: inspiration = \"AI\"\n",
    "        del ai_photo\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(\"WEB PHOTOS\")\n",
    "        for web_photo in web:\n",
    "            similarity = dino.calculate_similarity(final_photo, web_photo)\n",
    "            print(f\"Similarity: {similarity}\")\n",
    "            \n",
    "            min_web = min(min_web, similarity)\n",
    "            max_web = max(max_web, similarity)\n",
    "\n",
    "            draw_attention(dino, final_photo, web_photo)\n",
    "            draw_lines(dino, final_photo, web_photo, \"WEB\")\n",
    "\n",
    "            web_total += similarity\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                pic1, pic2 = final_photo, web_photo\n",
    "                if similarity > 0.5: inspiration = \"WEB\"\n",
    "        del web_photo\n",
    "        torch.cuda.empty_cache()\n",
    "        break\n",
    "\n",
    "    ai_total = ai_total / (len(final) * len(ai))\n",
    "    web_total = web_total / (len(final) * len(web))\n",
    "\n",
    "    del final_photo, final, ai, web\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f'\\tSimilarity scores - AI: {ai_total:.3f}\\tWEB: {web_total:.3f}')\n",
    "    print(f'\\tAI similarity - MAX: {max_ai} | MIN: {min_ai}')\n",
    "    print(f'\\tWEB similarity - MAX: {max_web} | MIN: {min_web}')\n",
    "    print(f'\\tAccording to DINO, this group was mostly inspired by {inspiration}.')\n",
    "        \n",
    "    if len(pic1) != 0 and len(pic2) != 0:\n",
    "        draw_attention(dino, pic1, pic2)\n",
    "        draw_lines(dino, pic1, pic2, inspiration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_results = pd.DataFrame(columns=['final_photo', 'inspiration', 'similarity'])\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i in range(len(final_data)):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        ai = final_data.iloc[i].ai_images\n",
    "        web = final_data.iloc[i].web_images\n",
    "        final = final_data.iloc[i].final_submissions\n",
    "        \n",
    "        ai_total, web_total = 0, 0\n",
    "        max_similarity, pic1, pic2 = -1, \"\", \"\"\n",
    "        min_similarity, pic3, pic4 = float('inf'), \"\", \"\"\n",
    "        inspiration = \"INCONCLUSIVE\"\n",
    "\n",
    "        print(f'Calculating similarity for group {final_data.iloc[i].group_code}')\n",
    "\n",
    "        for final_photo in final:\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            for ai_photo in ai:\n",
    "                similarity = dino.calculate_similarity(final_photo, ai_photo)\n",
    "                new_row = pd.DataFrame({'final_photo': [final_photo], 'inspiration': [ai_photo], 'similarity': [similarity]})\n",
    "                similarity_results = pd.concat([similarity_results, new_row], ignore_index=True)\n",
    "\n",
    "                ai_total += similarity\n",
    "                \n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    pic1, pic2 = final_photo, ai_photo\n",
    "                    if similarity > 0.5: inspiration = \"AI\"\n",
    "                if similarity < min_similarity:\n",
    "                    min_similarity = similarity\n",
    "                    pic3, pic4 = final_photo, ai_photo\n",
    "            \n",
    "            del ai_photo\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            for web_photo in web:\n",
    "                similarity = dino.calculate_similarity(final_photo, web_photo)\n",
    "                new_row = pd.DataFrame({'final_photo': [final_photo], 'inspiration': [web_photo], 'similarity': [similarity]})\n",
    "                similarity_results = pd.concat([similarity_results, new_row], ignore_index=True)\n",
    "\n",
    "                web_total += similarity\n",
    "                \n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    pic1, pic2 = final_photo, web_photo\n",
    "                    if similarity > 0.5: inspiration = \"WEB\"\n",
    "                if similarity < min_similarity:\n",
    "                    min_similarity = similarity\n",
    "                    pic3, pic4 = final_photo, web_photo\n",
    "\n",
    "            del web_photo\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        ai_total = ai_total / (len(final) * len(ai))\n",
    "        web_total = web_total / (len(final) * len(web))\n",
    "\n",
    "        del final_photo, final, ai, web\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f'\\tSimilarity scores - AI: {ai_total:.3f}\\tWEB: {web_total:.3f}')\n",
    "        print(f'\\tAccording to DINO, this group was mostly inspired by {inspiration}.')\n",
    "        \n",
    "        if len(pic1) != 0 and len(pic2) != 0:\n",
    "            draw_attention(dino, pic1, pic2)\n",
    "            draw_lines(dino, pic1, pic2, inspiration)\n",
    "\n",
    "similarity_results.to_csv('similarity_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
